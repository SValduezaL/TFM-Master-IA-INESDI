{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Para qué sirve el modelo?\n",
    "El modelo DETR ResNet-101 sirve para la detección de objetos en imágenes, identificando y localizando múltiples objetos dentro de una imagen.\n",
    "\n",
    "¿Quién o qué empresa ha entrenado el modelo?\n",
    "El modelo ha sido entrenado por Facebook AI (actualmente conocido como Meta AI).\n",
    "\n",
    "¿Con qué datos ha sido entrenado el modelo?\n",
    "El modelo ha sido entrenado con el conjunto de datos COCO 2017, que es un gran conjunto de imágenes etiquetadas para la detección de objetos, segmentación y más.\n",
    "\n",
    "¿Qué posible salida puede devolver el modelo?\n",
    "El modelo devuelve una lista de objetos detectados en la imagen, incluyendo sus clases (por ejemplo, \"perro\", \"bicicleta\") y las coordenadas de las cajas delimitadoras que indican su posición en la imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "#Se importan las librerías necesarias\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "\n",
    "# Se verifica uso de GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected cat with confidence 0.998 at location [344.06, 24.85, 640.34, 373.74]\n",
      "Detected remote with confidence 0.997 at location [328.13, 75.93, 372.81, 187.66]\n",
      "Detected remote with confidence 0.997 at location [39.34, 70.13, 175.56, 118.78]\n",
      "Detected cat with confidence 0.998 at location [15.36, 51.75, 316.89, 471.16]\n",
      "Detected couch with confidence 0.995 at location [-0.19, 0.71, 639.73, 474.17]\n"
     ]
    }
   ],
   "source": [
    "#Se carga el modelo y el procesador\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# you can specify the revision tag if you don't want the timm dependency\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-101\", revision=\"no_timm\")\n",
    "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-101\", revision=\"no_timm\")\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# convert outputs (bounding boxes and class logits) to COCO API\n",
    "# let's only keep detections with score > 0.9\n",
    "target_sizes = torch.tensor([image.size[::-1]])\n",
    "results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
    "\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(\n",
    "            f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
    "            f\"{round(score.item(), 3)} at location {box}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected person with confidence 0.999 at location [381.7, 136.93, 465.25, 417.76]\n",
      "Detected umbrella with confidence 0.999 at location [194.92, 80.89, 327.29, 211.31]\n",
      "Detected person with confidence 0.999 at location [207.91, 128.04, 286.73, 406.64]\n",
      "Detected umbrella with confidence 0.998 at location [54.14, 72.38, 207.28, 207.45]\n",
      "Detected person with confidence 0.995 at location [298.2, 143.06, 378.06, 413.69]\n",
      "Detected umbrella with confidence 0.998 at location [346.83, 99.26, 478.11, 222.89]\n",
      "Detected handbag with confidence 0.979 at location [264.1, 187.96, 284.92, 233.42]\n",
      "Detected umbrella with confidence 0.99 at location [464.02, 90.11, 603.31, 216.48]\n",
      "Detected person with confidence 0.999 at location [454.08, 140.33, 534.55, 421.04]\n",
      "Detected umbrella with confidence 0.994 at location [258.02, 227.61, 426.74, 347.87]\n",
      "Detected car with confidence 0.99 at location [33.45, 300.07, 105.07, 321.56]\n",
      "Detected person with confidence 1.0 at location [96.12, 95.84, 197.52, 403.26]\n"
     ]
    }
   ],
   "source": [
    "#Se seleccionan imagenes al azar para probar el modelo\n",
    "url = \"http://images.cocodataset.org/val2017/000000281759.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "#Se procesa la imagen\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "#Se convierten las salidas a la API de COCO\n",
    "target_sizes = torch.tensor([image.size[::-1]])\n",
    "results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
    "\n",
    "#Se imprimen los resultados\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(\n",
    "            f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
    "            f\"{round(score.item(), 3)} at location {box}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Para qué sirve el modelo?\n",
    "El modelo DETR (DEtection TRansformer) con la arquitectura ResNet-101 se utiliza para la detección de objetos en imágenes. Este proceso consiste en identificar automáticamente los diferentes objetos que aparecen en una imagen y determinar su ubicación exacta dentro de ella. Es decir, el modelo no solo reconoce qué objetos están presentes, sino que también traza un cuadro delimitador (bounding box) alrededor de cada uno, especificando sus coordenadas.\n",
    "\n",
    "Esta tecnología es esencial en numerosos campos de la visión por computadora, donde es necesario procesar grandes cantidades de imágenes para obtener información relevante sobre los objetos que contienen. Por ejemplo, en la seguridad y vigilancia, puede ser utilizado para detectar personas o vehículos en las grabaciones de cámaras de seguridad. En el ámbito médico, puede ayudar a los radiólogos a identificar y localizar anomalías en imágenes médicas como radiografías o resonancias magnéticas. Asimismo, en la conducción autónoma, es clave para que los vehículos reconozcan y reaccionen a su entorno, detectando otros coches, peatones, señales de tráfico, y obstáculos en la carretera.\n",
    "\n",
    "¿Quién o qué empresa ha entrenado el modelo?\n",
    "El modelo fue entrenado por Facebook AI Research (FAIR), ahora conocido como Meta AI, anteriormente Facebook. FAIR es un equipo de científicos especializados en inteligencia artificial que trabajan para Meta, con el objetivo de desarrollar tecnologías que permitan a las máquinas \"ver\" y \"comprender\" el entorno de manera similar a como lo hacemos los seres humanos. Para crear DETR, estos expertos integraron diversas técnicas de inteligencia artificial, algunas previamente utilizadas en otros campos como el análisis de texto, y las adaptaron para la detección de objetos en imágenes. Su enfoque es abierto, compartiendo sus investigaciones y liberando código y herramientas, como PyTorch, como PyTorch, para la comunidad.\n",
    "\n",
    "¿Con qué datos ha sido entrenado el modelo?\n",
    "El modelo ha sido entrenado utilizando el conjunto de datos COCO 2017 (Common Objects in Context), que contiene aproximadamente 118,000 imágenes etiquetadas. Cada imagen viene acompañada de información detallada sobre los objetos que aparecen en ella. Por ejemplo, si en una imagen hay un perro, un coche y una persona, cada uno de estos objetos estará identificado con su respectiva etiqueta (como \"perro\", \"coche\" o \"persona\"), y se indican las coordenadas exactas que definen un cuadro delimitador (bounding box) alrededor de cada objeto. El conjunto de datos incluye más de 80 categorías de objetos diferentes, desde animales y vehículos hasta utensilios de cocina y señales de tráfico, asegurando que el modelo pueda aprender a detectar una amplia variedad de objetos en diversos contextos y situaciones.  A medida que procesa miles de imágenes, ajusta sus parámetros internos para mejorar su capacidad de identificar correctamente los objetos y predecir sus ubicaciones en nuevas imágenes, lo que es esencial para su precisión y eficacia en aplicaciones del mundo real.\n",
    "\n",
    "\n",
    "\n",
    "¿Qué posible salida puede devolver el modelo?\n",
    "El modelo DETR (DEtection TRansformer) puede generar múltiples salidas al analizar una imagen. \n",
    "- Clases de los objetos detectados: El modelo identifica los tipos de objetos que aparecen en la imagen. Por ejemplo, podría reconocer un \"gato\", un \"coche\" o una \"persona\". Cada objeto detectado se asocia con una etiqueta de clase que indica su naturaleza.\n",
    "- Puntuaciones de confianza: Para cada objeto detectado, el modelo proporciona una puntuación que refleja su grado de certeza sobre la detección. Esta puntuación de confianza varía entre 0 y 1, donde un valor cercano a 1 indica que el modelo está muy seguro de haber identificado correctamente el objeto. Por ejemplo, podría afirmar que está un 99% seguro de que un objeto es un \"gato\".\n",
    "- Cuadros delimitadores (bounding boxes):\n",
    "El modelo también devuelve las coordenadas que describen un cuadro alrededor de cada objeto detectado en la imagen. Estas coordenadas definen un rectángulo (el cuadro delimitador) que indica con precisión la ubicación del objeto. Este rectángulo se representa mediante cuatro números que especifican los bordes del cuadro: las posiciones de los lados izquierdo, superior, derecho e inferior.\n",
    "\n",
    "Por ejemplo, si el modelo analiza una imagen que muestra una cocina en la que hay una mesa con una manzana, un plato, y una jarra de agua. El modelo podría devolver resultados como los siguientes:\n",
    "- Objeto 1: Manzana, con una confianza del 98.5%, ubicado en el cuadro delimitador [150.32, 120.45, 200.67, 180.89].\n",
    "- Objeto 2: Plato, con una confianza del 97.2%, ubicado en el cuadro delimitador [220.12, 140.67, 300.45, 220.93].\n",
    "- Objeto 3: Jarra de agua, con una confianza del 95.0%, ubicada en el cuadro delimitador [310.50, 80.30, 370.15, 180.75].\n",
    "\n",
    "Esta información es vital en aplicaciones como la automatización del hogar, donde se necesita reconocer y gestionar elementos en el entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c5d5bade79e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Se importan las librerías necesarias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDetrImageProcessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDetrForObjectDetection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "#Se importan las librerías necesarias\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "\n",
    "# Se verifica uso de GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected cat with confidence 0.998 at location [344.06, 24.85, 640.34, 373.74]\n",
      "Detected remote with confidence 0.997 at location [328.13, 75.93, 372.81, 187.66]\n",
      "Detected remote with confidence 0.997 at location [39.34, 70.13, 175.56, 118.78]\n",
      "Detected cat with confidence 0.998 at location [15.36, 51.75, 316.89, 471.16]\n",
      "Detected couch with confidence 0.995 at location [-0.19, 0.71, 639.73, 474.17]\n"
     ]
    }
   ],
   "source": [
    "#Se carga el modelo y el procesador\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# you can specify the revision tag if you don't want the timm dependency\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-101\", revision=\"no_timm\")\n",
    "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-101\", revision=\"no_timm\")\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# convert outputs (bounding boxes and class logits) to COCO API\n",
    "# let's only keep detections with score > 0.9\n",
    "target_sizes = torch.tensor([image.size[::-1]])\n",
    "results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
    "\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(\n",
    "            f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
    "            f\"{round(score.item(), 3)} at location {box}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected person with confidence 0.999 at location [381.7, 136.93, 465.25, 417.76]\n",
      "Detected umbrella with confidence 0.999 at location [194.92, 80.89, 327.29, 211.31]\n",
      "Detected person with confidence 0.999 at location [207.91, 128.04, 286.73, 406.64]\n",
      "Detected umbrella with confidence 0.998 at location [54.14, 72.38, 207.28, 207.45]\n",
      "Detected person with confidence 0.995 at location [298.2, 143.06, 378.06, 413.69]\n",
      "Detected umbrella with confidence 0.998 at location [346.83, 99.26, 478.11, 222.89]\n",
      "Detected handbag with confidence 0.979 at location [264.1, 187.96, 284.92, 233.42]\n",
      "Detected umbrella with confidence 0.99 at location [464.02, 90.11, 603.31, 216.48]\n",
      "Detected person with confidence 0.999 at location [454.08, 140.33, 534.55, 421.04]\n",
      "Detected umbrella with confidence 0.994 at location [258.02, 227.61, 426.74, 347.87]\n",
      "Detected car with confidence 0.99 at location [33.45, 300.07, 105.07, 321.56]\n",
      "Detected person with confidence 1.0 at location [96.12, 95.84, 197.52, 403.26]\n"
     ]
    }
   ],
   "source": [
    "#Se seleccionan imagenes al azar para probar el modelo\n",
    "url = \"http://images.cocodataset.org/val2017/000000281759.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "#Se procesa la imagen\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "#Se convierten las salidas a la API de COCO\n",
    "target_sizes = torch.tensor([image.size[::-1]])\n",
    "results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
    "\n",
    "#Se imprimen los resultados\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(\n",
    "            f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
    "            f\"{round(score.item(), 3)} at location {box}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
